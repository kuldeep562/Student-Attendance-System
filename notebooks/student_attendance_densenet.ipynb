{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133d8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def detect_and_save_faces(image_dir, output_dir):\n",
    "    # Load OpenCV's pre-trained face detection model\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Loop through each person's folder\n",
    "    for person in os.listdir(image_dir):\n",
    "        person_dir = os.path.join(image_dir, person)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "\n",
    "        person_output_dir = os.path.join(output_dir, person)\n",
    "        if not os.path.exists(person_output_dir):\n",
    "            os.makedirs(person_output_dir)\n",
    "\n",
    "        # Loop through each image in the person's folder\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, image_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "            # Save the detected faces\n",
    "            for (x, y, w, h) in faces:\n",
    "                face_img = img[y:y+h, x:x+w]\n",
    "                face_img_path = os.path.join(person_output_dir, image_name)\n",
    "                cv2.imwrite(face_img_path, face_img)\n",
    "\n",
    "# Specify your directories\n",
    "image_dir = r\"C:\\Users\\mitpa\\OneDrive\\Desktop\\hrs\\face detection project\\DataSet Sept-Dec2023\\data attendence 031023\\Photoes\"\n",
    "\n",
    "output_dir = 'output_faces'\n",
    "\n",
    "# Detect and save faces\n",
    "detect_and_save_faces(image_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e2d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mitpa\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc8d3e",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac88514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mitpa\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mitpa\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the DenseNet121 model without the top layer\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c55421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of DenseNet121\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(40, activation='softmax')(x)  # 40 classes for 40 folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9047cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa359d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model layers so they are not updated during training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa9599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mitpa\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with an optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46641051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation and data generator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d993c560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 40 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'output_faces',  # Path to the pre-processed face images\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a56f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 40 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'output_faces',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eab25bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 2.0493 - accuracy: 0.4787 - val_loss: 2.8859 - val_accuracy: 0.2812\n",
      "Epoch 2/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.9224 - accuracy: 0.5183 - val_loss: 2.8238 - val_accuracy: 0.3594\n",
      "Epoch 3/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.9405 - accuracy: 0.5122 - val_loss: 2.8732 - val_accuracy: 0.3438\n",
      "Epoch 4/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.7726 - accuracy: 0.5457 - val_loss: 2.8009 - val_accuracy: 0.3594\n",
      "Epoch 5/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.5932 - accuracy: 0.6037 - val_loss: 2.6695 - val_accuracy: 0.3750\n",
      "Epoch 6/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.5990 - accuracy: 0.5579 - val_loss: 2.9191 - val_accuracy: 0.3438\n",
      "Epoch 7/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.4964 - accuracy: 0.5976 - val_loss: 2.8861 - val_accuracy: 0.3906\n",
      "Epoch 8/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.4208 - accuracy: 0.6189 - val_loss: 2.9153 - val_accuracy: 0.3281\n",
      "Epoch 9/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3626 - accuracy: 0.6280 - val_loss: 2.8986 - val_accuracy: 0.3438\n",
      "Epoch 10/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.3206 - accuracy: 0.6677 - val_loss: 2.9355 - val_accuracy: 0.3281\n",
      "Epoch 11/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 1.2018 - accuracy: 0.6799 - val_loss: 2.9291 - val_accuracy: 0.3438\n",
      "Epoch 12/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.1116 - accuracy: 0.7134 - val_loss: 2.9493 - val_accuracy: 0.3750\n",
      "Epoch 13/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.1163 - accuracy: 0.7287 - val_loss: 2.9442 - val_accuracy: 0.3125\n",
      "Epoch 14/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.1188 - accuracy: 0.7317 - val_loss: 2.9774 - val_accuracy: 0.3438\n",
      "Epoch 15/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 1.0424 - accuracy: 0.7835 - val_loss: 2.9851 - val_accuracy: 0.3125\n",
      "Epoch 16/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.9601 - accuracy: 0.7744 - val_loss: 3.0396 - val_accuracy: 0.3594\n",
      "Epoch 17/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.9647 - accuracy: 0.7470 - val_loss: 3.0879 - val_accuracy: 0.3281\n",
      "Epoch 18/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.9176 - accuracy: 0.7530 - val_loss: 3.1005 - val_accuracy: 0.3281\n",
      "Epoch 19/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.8773 - accuracy: 0.7652 - val_loss: 2.9731 - val_accuracy: 0.3750\n",
      "Epoch 20/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7930 - accuracy: 0.8171 - val_loss: 2.9254 - val_accuracy: 0.4062\n",
      "Epoch 21/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.7627 - accuracy: 0.8384 - val_loss: 2.9261 - val_accuracy: 0.3906\n",
      "Epoch 22/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7215 - accuracy: 0.8354 - val_loss: 3.1695 - val_accuracy: 0.3438\n",
      "Epoch 23/25\n",
      "11/11 [==============================] - 12s 1s/step - loss: 0.7009 - accuracy: 0.8476 - val_loss: 2.9022 - val_accuracy: 0.3438\n",
      "Epoch 24/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.7240 - accuracy: 0.8097 - val_loss: 3.0459 - val_accuracy: 0.3438\n",
      "Epoch 25/25\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.6979 - accuracy: 0.8182 - val_loss: 3.3056 - val_accuracy: 0.2969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b4a7fa9010>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25,\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    validation_steps=validation_generator.samples // 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae1c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitpa\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model for future use\n",
    "model.save('face_recognition_densenet121_labels.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300d3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No recognizable face found in the selected segments.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the trained DenseNet121 model\n",
    "model = load_model('face_recognition_densenet121_labels.h5')\n",
    "\n",
    "# Define the labels (1 to 40) corresponding to the folders\n",
    "labels = list(range(1, 41))\n",
    "\n",
    "# Initialize an empty list to store attendance records\n",
    "attendance = []\n",
    "\n",
    "# Function to mark attendance\n",
    "def mark_attendance(label):\n",
    "    time_now = datetime.now()\n",
    "    attendance.append({'Roll Number': label, 'Time': time_now.strftime('%H:%M:%S'), 'Date': time_now.strftime('%Y-%m-%d')})\n",
    "\n",
    "# Function to save attendance to an Excel file\n",
    "def save_attendance():\n",
    "    df = pd.DataFrame(attendance)\n",
    "    df.to_excel('attendance.xlsx', index=False)\n",
    "\n",
    "# Function to check student presence in randomly selected segments\n",
    "def check_student_presence(video_path, num_segments=10, threshold=7):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    # Calculate segment length (in seconds)\n",
    "    segment_length = duration / num_segments\n",
    "\n",
    "    # Randomly select 10 segments from the video\n",
    "    segments = sorted(random.sample(range(num_segments), num_segments))\n",
    "\n",
    "    presence_count = 0\n",
    "    recognized_label = None\n",
    "\n",
    "    for segment in segments:\n",
    "        # Set the video to the start of the segment\n",
    "        start_frame = int(segment * segment_length * fps)\n",
    "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Convert the frame to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        # Check if the student's face is recognized\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (224, 224))\n",
    "            face_array = img_to_array(face_img)\n",
    "            face_array = np.expand_dims(face_array, axis=0) / 255.0\n",
    "\n",
    "            # Predict the identity of the face\n",
    "            predictions = model.predict(face_array)\n",
    "            max_index = np.argmax(predictions[0])\n",
    "            recognized_label = labels[max_index]\n",
    "\n",
    "            # If face is recognized, increase presence count\n",
    "            presence_count += 1\n",
    "            break  # Only count the first recognized face in the segment\n",
    "\n",
    "    # Determine if the student is present or absent\n",
    "    if recognized_label is not None:\n",
    "        if presence_count >= threshold:\n",
    "            mark_attendance(recognized_label)\n",
    "            print(f\"Student with Roll Number {recognized_label} is Present\")\n",
    "        else:\n",
    "            print(f\"Student with Roll Number {recognized_label} is Absent\")\n",
    "    else:\n",
    "        print(\"No recognizable face found in the selected segments.\")\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "# Run the presence check on your video\n",
    "check_student_presence(r\"C:\\Users\\mitpa\\OneDrive\\Desktop\\hrs\\face detection project\\DataSet Sept-Dec2023\\CCTV Footage 30_09_2023\\22.9.23 class 1\\172.16.116.247_Class_1_1_main_20230922130004_140004.asf\")\n",
    "\n",
    "# Save the attendance records when finished\n",
    "save_attendance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
